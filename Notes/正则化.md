# 正则化
### 简介
正则化是用于解决过拟合问题的一种方法。过拟合是指模型在训练数据上表现得很好，但在测试数据上表现很差。正则化通过在损失函数中加入一个惩罚项，来限制模型的复杂度，从而提高模型的泛化能力。
### Batch Normalization批量标准化
调整各层激活值的分布使其拥有适当的广度（分散开），有时候简称为BN层，通常放在线性层(全连接层/卷积层)之后
- 可以是学习快速进行(允许更高的学习率)
- 不那么依赖初始值
- 抑制过拟合

Batch Normalization会先对数据进行标准化，之后在对数据进行缩放和平移
#### 1. 均值（Mean）
$$
\mu = \frac{1}{n} \sum x
$$

#### 2. 方差（Variance）
$$
\sigma^2 = \frac{1}{n} \sum (x - \mu)^2
$$

#### 3. 标准化（Normalization）
$$
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \varepsilon}}
$$

其中，$\varepsilon$ 是一个很小的常数，用于防止分母为 0。

#### 4. 缩放与平移（Scale and Shift）
$$
y = \gamma \hat{x} + \beta
$$

### 参数说明

- $\varepsilon$：一个很小的常数，防止分母为 0  
- $\gamma$：缩放参数（scale），可通过学习得到  
- $\beta$：偏置参数（shift），可通过学习得到  


### 权值衰减
通过在学习过程中对大权重进行“惩罚”，可以有效地抑制过拟合。这种方法也被称为**权值衰减**，因为很多过拟合产生的原因是模型参数过大。

一般会在损失函数中加入一个关于权重的惩罚项，最常见的是 **L2 正则化（权重平方和）**：

$$
L' = L + \frac{1}{2}\lambda \lVert W \rVert^2
$$

---

#### 各项含义说明

- $L$：原始损失函数  
- $L'$：加入正则化后的损失函数  
- $\lVert W \rVert^2$：权重 $W = (w_1, w_2, \dots, w_n)$ 的 L2 范数平方，即  
  $$
  \lVert W \rVert^2 = w_1^2 + w_2^2 + \cdots + w_n^2
  $$
- $\lambda$：正则化强度的超参数，用于控制惩罚项的大小  

---

#### 梯度更新中的影响

在反向传播时，正则项对权重的梯度为：

$$
\frac{\partial}{\partial W} \left( \frac{1}{2}\lambda \lVert W \rVert^2 \right) = \lambda W
$$

因此，在更新权重时，相当于在原有梯度的基础上**额外加上一个 $\lambda W$**，使得权重在每次更新时都会被向 0 拉回，从而防止权重过大。

### Dropout随机失活

Dropout（随机失活，暂退法）是一种在学习过程中**随机关闭部分神经元**的方法。

在训练时，以概率 $p$ 随机关闭神经元，迫使网络不依赖某些特定神经元，从而增强模型的鲁棒性。同时，对**未被关闭的神经元输出按 $\frac{1}{1-p}$ 的比例进行缩放**，以保持输出的期望值不变。

在测试阶段，**不使用 Dropout**，所有神经元均处于激活状态，并且不再进行缩放。

---

#### 训练阶段
- 神经元以概率 $p$ 被随机失活  
- 未失活的神经元输出乘以 $\frac{1}{1-p}$  
- 保持激活值的期望不变  

#### 测试阶段
- 所有神经元均参与计算  
- 不再进行随机失活和缩放  

---

#### Dropout 的作用

- 防止神经元之间的过度依赖  
- 提高模型的泛化能力  
- 具有类似**集成学习（Ensemble）**的效果  
  - 每次训练相当于使用不同的子网络  
  - 测试时近似等价于对多个模型取平均  

---

#### 使用位置

- Dropout 可用于**全连接层和卷积层**
- 对大规模网络效果尤为明显
- 通常放置在：
  - **激活函数之后**
  - **线性层（全连接层 / 卷积层）之前**