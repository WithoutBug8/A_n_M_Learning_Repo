# 深度神经网络
### 简介
当神经网络的层数加深时,往往可以有效地提高识别精度;特别是对于大规模的复杂问题,通常都需要用更多的层来分层次传递信息,而且可以有效减少网络的参数数量,从而使得学习更加高效。使用深度神经网络进行的学习就被称为深度学习(Deep Learning)。
### 梯度消失和梯度爆炸
- **梯度消失**是随着网络深度的增加，梯度在隐藏层反向传播时倾向于变小。这就意味着，前面的隐藏层神经元要比后面的学习起来更慢，这种现象叫做**梯度消失**
- **梯度爆炸**与之对应的是，如果我们进行一些特殊的调整（比如初始权重给的很大），可以让梯度反向传播时不会明显减小，从而解决梯度消失的问题，然而这样一来，前面层的梯度又会变得很大，
引起网络的不稳定，无法再从训练数据中学习，我们称这种现象叫做**梯度爆炸**
- `我们训练的过程中主要会遇到的问题时，梯度消失。所以可见神经网络的训练的过程中还是挺复杂的~`

### 学习算法（更新参数方法的优化）
1. SGD方法

SGD的更新方法可以写做：
$W \leftarrow W - \eta\frac{\partial L}{\partial W}$
这里W是更新的权重参数矩阵，损失函数L，$\eta$是学习率
`以下全是缺点`
- 陷入局部最优解，尤其在非凸函数中，难以找到全局最优解
- 陷入鞍点(导数为0但不是极小值点)，梯度为0，导致训练停滞
- 收敛速度慢，高维或者非凸函数中，收敛速度较慢
- 学习率过大导致震荡或者不收敛，过小则收敛速度过慢
2. Momentum(动量法)
- 这个方法解决的是上面缺点2，遇到驻点给一个冲力，防止一直陷入鞍点，导致训练停滞
动量法有时能够**减缓**优化过程中的震荡，也可**加快**优化速度。
公式如下
$\begin{align}
v &\leftarrow \alpha v - \eta \nabla (1) \\
W &\leftarrow W + v (2)
\end{align}
$
- $v$ 是历史（负）梯度的加权和
- $\alpha$ 历史梯度权重
- $\nabla$ 是当前的梯度
- $\eta$ 是学习率
3. 学习率衰减
- 深度学习模型训练中调整最频繁的当属学习率,好的学习率可以使模型逐渐收敛并获得更好的精度。较大的学习率可以加快收敛速度,但可能在最优解附近震荡或不收敛;
较小的学习率可以提高收敛的精度,但训练速度慢。
学习率衰减是一种平衡策略,初期使用较大学习率快速接近最优解,后期逐渐减小学习率,使参数更稳定地收敛到最优解。
- 
4. AdaGrad
5. RMSProp
6. Adam