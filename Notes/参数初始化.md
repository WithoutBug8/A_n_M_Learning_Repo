# 参数初始化

### 常数初始化
将所有的权重参数初始化为一个常数，记得**千万要避免不要把全部的参数赋值为0**
神经网络的不同权重意义丧失了，导致了*权重均一化*，请随机生成初始值

### 秩初始化
当前的权重矩阵定义为单位矩阵（主对角线元素为1，其余元素为0）

### 正态分布初始化
权重参数按照指定的均值 $\mu$和标准差 $\sigma$

### 均匀分布初始化
权重参数按照指定的范围随机(a,b)生成，均匀分布一般记作$ X \sim U(a,b)$

### Xavier初始化（也叫Glorot初始化）
根据输入和输出的神经元数量调整权重的初始范围，确保每一层的输出方差与输入方差相近
- Xavier正态分布初始化：均值为0，标准差为$\sqrt{\frac{2}{n_{in}+n_{out}}}$
- Xavier均匀分布初始化：范围为$[-\sqrt{\frac{6}{n_{in}+n_{out}}},\sqrt{\frac{6}{n_{in}+n_{out}}}]$
- 其中$n_{in}$是输入神经元的数量，$n_{out}$是输出神经元的数量

Xavier初始化参数适用于sigmoid和Tanh等激活函数，能有效缓解梯度消失和梯度爆炸问题

### He初始化（何恺明初始化）
He初始化根据输入神经元的数量调整权重的初始范围
- He正态分布初始化：均值为0，标准差为$\sqrt{\frac{2}{n_{in}}}$
- He均匀分布初始化：范围为$[-\sqrt{\frac{6}{n_{in}}},\sqrt{\frac{6}{n_{in}}}]$
- 其中$n_{in}$是输入神经元的数量

He初始化参数适用于ReLU等激活函数
