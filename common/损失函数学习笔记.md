# 这是损失函数Loss function的学习笔记

## 定义
损失函数（Loss Function） 是用于衡量模型预测结果与真实标签之间差距的函数。
在训练神经网络时，我们希望模型的预测越接近真实值越好，因此目标是：
$Minimize L = Loss(y,t)$
其中：

- 𝑦 模型的预测值

- 𝑡 真实标签（target）

- 𝐿 损失（loss），表示模型预测的误差大小

常见例子：

平方误差损失（MSE）：$L = (y - t)^2$

交叉熵损失（Cross-Entropy）：用于分类任务，衡量预测分布与真实分布的差异。
## 误区
学习神经网络时，经常会把神经元计算公式与损失函数混淆。

神经元的计算公式是：
$ y = Wx + b $

而损失函数是：
$L = Loss(y,t)$

⚠️ 区别：
$ y = Wx + b $是前向传播（forward propagation）的一部分，用来计算预测结果。

𝐿 是模型性能的度量，用来告诉我们预测得好不好。

所以：

神经元公式描述“模型如何计算输出”，
损失函数描述“这个输出离真相有多远”。

## 损失函数和导数之间的关系

神经网络能够“学习”的主要原因在于**导数Derivative**

1. 导数变换率：告诉我们当参数 $W$ 变化时，损失 $L$ 如何变化

   $\frac{\partial L}{\partial W}$

   代表如果我稍微改动一下W，损失会变大还是变小

2. 梯度下降法Gradient Descent

   为了让损失更小，我们沿着损失下降最快的方向更新参数“

   $W \leftarrow W - \eta\frac{\partial L}{\partial W}$

​	其中 $\eta$ 是学习率(learning rate)，控制步长

3. 导数符号的具体含义(**反向传播过程中**)
   - 若 $\frac{\partial L}{\partial W} > 0$ : 说明当前是上坡，斜率大于零，应该减小W才能下坡；
   -  若 $\frac{\partial L}{\partial W} < 0$ : 说明当前是下坡，斜率小于零，应该增加W才能上坡；
   - 一句话总结来说：**我们把损失函数想象成一条山坡曲线，导数表示当前的位置的坡度方向，在参数更新阶段，我们想让损失越来越小，因此我们要沿着导数的反方向调整参数**
