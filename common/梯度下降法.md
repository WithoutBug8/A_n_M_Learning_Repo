# 梯度下降法(Gradient Descent)
梯度下降法(Gradient Descent)是一种用于最小化目标函数的迭代优化算法。核心是沿着目标函数(如损失函数)的负梯度方向逐步调整参数,从而逼近函数的最小值。梯度方向指示了函数增长最快的方向,因此负梯度方向是函数下降最快的方向。
具体来说,我们初始找到函数f(x1,x2)的一个点(x1,x2),按下式进行更新:

- $x_1' = x_1 - \eta\frac{\partial f}{\partial x_1}$ 
- $x_2' = x_2 - \eta\frac{\partial f}{\partial x_2}$

这样就可以沿着负梯度的方向，找到一个新的点($x_1', x_2'$) , 让函数值更小；

其中$\eta$ 表示每次的更新量，在神经网络的学习过程中，就代表了一次学习的步长，称为学习率(Learning Rate) 这是一个**超参数** 需要我们设置，过大或者过小都会导致学习效果不佳。

### 一些定义

1. **Epoch**
   **1个Epoch 表示模型完整遍历一次整个训练数据集的过程**。

   例如,训练10个Epoch 表示模型将整个数据集反复学习10次。模型需要多次遍历数据集(多个Epoch)才能逐步学习数据中的模式,单次遍历数据集 (1个Epoch)通常不足以让模型收敛,多次遍历可以逐步优化模型参数。

2. Batch Size
  Batch Size 是每次训练时输入的样本数量。例如,Batch Size=32 表示每次用32个样本计算一次梯度并更新模型参数。

  小批量数据计算梯度比单样本(Batch Size=1)更稳定,比全批量(Batch Size=全体数据)更高效。并且较小的Batch Size可能带来更多噪声,有助于模型泛化。

3. Iteration
  **一次 Iteration 表示完成一个Batch 数据的正向传播(预测)和反向传播(更新参数)的过程**。

  和**Epoch**的区别是完成一次小的数据的传播过程

  例如,数据集现有2000个样本,对其训练10个Epoch,选择 Batch Size=64:
  Batch个数为$2000 \div 64+1=31+1=32个$(最后一个Batch仅有16个样本)。
  每个Epoch 中迭代次数 Iteration=32次。
  总迭代次数为10×32=320次。
  总训练样本数为 10 * 2000 = 20000

### 随机梯度下降法(SGD)

在神经网络的学习过程中,可以使用梯度下降法来更新参数,目标就是减小损失函数的值。
实际操作时,一般会从训练数据中随机选择一个小批量数据(mini-batch),然后用梯度下降法迭代多个轮次(iteration);这种“对随机选择的数据进行的梯度下降法”,被称作随机梯度下降法(stochastic gradient descent, SGD).
具体过程如下:

1. 随机选择批数据(mini-batch)
   从训练数据中随机选出一部分数据,学习的目标就是要减少这个mini-batch 数据的损失函数值。
2. 计算梯度
   对当前的各权重参数,计算出梯度的值,负梯度就表示了损失函数减小最多的方向。
3. 更新参数
   按照3.4.1 节中梯度下降法的公式,对权重参数沿负梯度方向进行微小更新。
4. 重复迭代
   重复上面的步骤1，2，3,直到完成预定的总迭代次数。
