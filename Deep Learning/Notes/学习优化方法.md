# 学习优化方法
### 简介
当神经网络的层数加深时,往往可以有效地提高识别精度;特别是对于大规模的复杂问题,通常都需要用更多的层来分层次传递信息,而且可以有效减少网络的参数数量,从而使得学习更加高效。使用深度神经网络进行的学习就被称为深度学习(Deep Learning)。
### 梯度消失和梯度爆炸
- **梯度消失**是随着网络深度的增加，梯度在隐藏层反向传播时倾向于变小。这就意味着，前面的隐藏层神经元要比后面的学习起来更慢，这种现象叫做**梯度消失**
- **梯度爆炸**与之对应的是，如果我们进行一些特殊的调整（比如初始权重给的很大），可以让梯度反向传播时不会明显减小，从而解决梯度消失的问题，然而这样一来，前面层的梯度又会变得很大，
引起网络的不稳定，无法再从训练数据中学习，我们称这种现象叫做**梯度爆炸**
- `我们训练的过程中主要会遇到的问题时，梯度消失。所以可见神经网络的训练的过程中还是挺复杂的~`

### 学习算法（更新参数方法的优化）
1. SGD方法(随机梯度下降)

SGD的更新方法可以写做：
$W \leftarrow W - \eta\frac{\partial L}{\partial W}$
这里W是更新的权重参数矩阵，损失函数L，$\eta$是学习率
`以下全是缺点`
- 陷入局部最优解，尤其在非凸函数中，难以找到全局最优解
- 陷入鞍点(导数为0但不是极小值点)，梯度为0，导致训练停滞
- 收敛速度慢，高维或者非凸函数中，收敛速度较慢
- 学习率过大导致震荡或者不收敛，过小则收敛速度过慢
2. Momentum(动量法)
- 这个方法解决的是上面缺点2，遇到驻点给一个冲力，防止一直陷入鞍点，导致训练停滞
动量法有时能够**减缓**优化过程中的震荡，也可**加快**优化速度。
公式如下
$\begin{align}
v &\leftarrow \alpha v - \eta \nabla (1) \\
W &\leftarrow W + v (2)
\end{align}
$
- $v$ 是历史（负）梯度的加权和
- $\alpha$ 历史梯度权重
- $\nabla$ 是当前的梯度
- $\eta$ 是学习率
3. 学习率衰减
- 深度学习模型训练中调整最频繁的当属学习率,好的学习率可以使模型逐渐收敛并获得更好的精度。较大的学习率可以加快收敛速度,但可能在最优解附近震荡或不收敛;
较小的学习率可以提高收敛的精度,但训练速度慢。
学习率衰减是一种平衡策略,初期使用较大学习率快速接近最优解,后期逐渐减小学习率,使参数更稳定地收敛到最优解。
- 等间隔衰减
  - 每隔固定的epoch，学习率按照一定的比例下降
- 指定间隔衰减
  - 在指定的epoch，学习率按照一定的系数衰减
- 指数衰减
  - 学习率按照指数函数$f(x) = a^x , a < 1$ 注意选择a的时候尽量靠近1，比如0.99，否则会衰减太快
4. AdaGrad
- Adaptive Gradient自适应梯度，为每个参数适当的调整学习率，并随着学习的进行，学习率逐渐减小
$
  h\leftarrow h + \nabla^2 (1)  \\
  w \leftarrow w - \frac{\eta}{\sqrt{h}}\nabla  (2)
$
- h: 是历史梯度的平方和
5. RMSProp
- Root Mean Square Propagation 均方根传播，是在AdaGrad上的一种改进，它并非将过去的梯度一视同仁的相加，而是逐渐遗忘过去的梯度，采用指数加权平均的方式
$
  h\leftarrow \alpha h + (1-\alpha)\nabla^2 (1)  \\
  w \leftarrow w - \frac{\eta}{\sqrt{h}}\nabla  (2)
$
- h是历史梯度平方和的指数移动加权平均
- $\alpha$ 权重
6. Adam
- Adaptive Moment Estimation自适应矩估计，它综合了动量法和AdaGrad的优点
$
    v \leftarrow \alpha_1 v + (1 - \alpha_1)\nabla \\
    h \leftarrow \alpha_2 h + (1 - \alpha_2)\nabla^2 \\
    \hat{v} = \frac{v}{1 - \alpha_1^t} \\
    \hat{h} = \frac{h}{1 - \alpha_2^t} \\
    W \leftarrow W - \eta \frac{\hat{v}}{\sqrt{\hat{h}}}
$
- **η**：学习率  
- **α₁、α₂**：一次动量系数和二次动量系数  
- **t**：迭代次数，从 1 开始