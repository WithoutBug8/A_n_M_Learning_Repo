# 反向传播算法(Backward Propagation)
### 简介
负责计算神经网络参数梯度的方法。简言之，该方法根据微积分中的**链式求导法则**，按照相反的顺序从输出层到输入层遍历网络，该算法存储了计算某些参数梯度时所需的任何中间变量

### 过程
局部导数向反方向传递，基于链式求导法则，依次计算导数值，这个过程称为反向传播。
反向传播时将信号乘以节点的局部导数然后传递给下一个节点。举例来说⬇
$z = (x+y)^2$ 令 $u = x + y$ 则 $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial u} \frac{\partial u}{\partial x} = 2u * 1 = 2(x + y)$
- 如果一个节点是**加法**，那么经过反向传播后，会将原来上游传来的值原封不动的传递给下游
- 如果一个节点是**乘法**，那么经过反向传播后，会将原来上游传来的值乘以输入的反向传播值向下游传递

### 激活层的反向传播和实现
具体代码实现在[反向传播函数](../common/layers.py)
1. `ReLU`函数的反向传播
$f(x)=max(0,x)= \left\{
  \begin{array}{ll}
    0, & x \le 0 \\
    x, & x > 0
  \end{array}
\right.$
所以它的导数不是0就是1,所以在实际情况中，只需要保存x>0的情况，这个不变，剩下的直接用0覆盖就可以了
2. `Sigmoid`函数的反向传播
原函数$f(x)=\frac{1}{1+e^{-x}}$
导函数$f'(x) = f(x)(1-f(x))$

### 全连接层(Affine)的反向传播和实现
在全连接层(Fully Connected Layer, Dense Layer)中,每个输入节点与输出节点相连, 通过权重矩阵和偏置进行线性变换,这种操作在几何领域称为仿射变换(Affine transformation,几何中,仿射变换包括一次线性变换和一次平移,分别对应神经网络的加权求和运算与加偏置运算)。

考虑N个数据一起进行正向传播的情况,写成矩阵计算形式: $Y = XW +B$
- 这里的X是$N \times m$的矩阵,m就是Affine层输入神经元的个数；
- W的形状是$m \times n$的权重矩阵，n就是Affine层输出神经元的个数；

根据矩阵求导的运算法则，我们可以得到损失函数L关于X和W的偏导数
$\frac{\partial L}{\partial X}=\frac{\partial L}{\partial Y} \cdot W^T$

$\frac{\partial L}{\partial W}=X^T \cdot \frac{\partial L}{\partial Y}$

### 输出层的反向传播和实现
在输出层，我们一般使用`Softmax`函数作为激活函数，对于Softmax函数
$
 y_k = \frac{e^{x_k}}{\sum_{i=1}^{n} e^{x_i}}, \quad k = 1, 2, \ldots, n
$
其导数为：
$
    \frac{\partial y_k}{\partial x_i} =
    \begin{cases}
    y_k(1 - y_i), & k = i \\
    -\,y_k y_i, & k \ne i
    \end{cases}
$
- 对于输出层，一般会直接将结果带入损失函数的计算，这里选择交叉熵函数(Cross Entropy Loss)作为损失函数，就可以得到一个Softmax-with-Loss层，
![具体原理图结果如这里所示](./pic/SoftmaxWithLoss.png)